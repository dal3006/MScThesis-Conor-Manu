{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"LGBM_datasize.ipynb","provenance":[{"file_id":"1uWw2wZyZ8yORKTYJ3fKRfEsKjYvuLcpa","timestamp":1598465352152},{"file_id":"1B3RwPaDkpiRYdoj7BB0XD4fa9lmK3xOk","timestamp":1596382012502}],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"y6YsUR3mUOEy","colab_type":"text"},"source":["# LGBM for the traffic flow data"]},{"cell_type":"markdown","metadata":{"id":"3d2qnLn3UOE1","colab_type":"text"},"source":["https://www.kaggle.com/robikscube/tutorial-time-series-forecasting-with-xgboost"]},{"cell_type":"markdown","metadata":{"id":"4jMJD4_iUOE4","colab_type":"text"},"source":["Importing libraries"]},{"cell_type":"code","metadata":{"id":"2KN6E8MKUOE7","colab_type":"code","colab":{}},"source":["# general routine set up:\n","\n","# ignore warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# get wd and change the wd plus import libraries\n","import os\n","os.getcwd()\n","#os.chdir(\"/Users/Manu/Dropbox/CBS MSc Thesis Research Folder/DATA & Code/Model Specific Notebooks\")\n","\n","\n","# standard\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt \n","%matplotlib inline\n","from math import sqrt\n","import pickle\n","\n","# ML\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import TimeSeriesSplit\n","from sklearn.model_selection import train_test_split\n","import xgboost as xgb\n","from xgboost import plot_importance, plot_tree\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","from sklearn.multioutput import MultiOutputRegressor\n","import tensorflow.keras.backend as K\n","from lightgbm import LGBMRegressor"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JDuBnHiZUOFN","colab_type":"text"},"source":["Loading the data"]},{"cell_type":"code","metadata":{"id":"aIizwmU9XRQK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1598524535085,"user_tz":-120,"elapsed":19765,"user":{"displayName":"Manuel S","photoUrl":"","userId":"12634565971130299273"}},"outputId":"5864f061-fdb5-411c-cde6-33dd4cc5c3cb"},"source":["# this allows for accessing files stored in your google drive using the path \"/gdrive/\"\n","# mounting google drive locally:\n","\n","from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9R3DJqolXY_e","colab_type":"code","colab":{}},"source":["#loading the hourly traffic data (1 year of data; June 2018 to June 2019)\n","filename = \"/gdrive/My Drive/Colab Notebooks/taxi_series_H\"\n","infile = open(filename,'rb')\n","taxidemand_ts = pickle.load(infile)\n","infile.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VWye-hnAzNq7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598469417436,"user_tz":-120,"elapsed":1084,"user":{"displayName":"Manuel S","photoUrl":"","userId":"12634565971130299273"}},"outputId":"2e533a54-1ce6-4b2e-9904-11e7ee2cc259"},"source":["taxidemand_ts.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8760,)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"Z-AofGAaUOFY","colab_type":"text"},"source":["Preprocessing: already done\n"]},{"cell_type":"markdown","metadata":{"id":"exVAFiNZUOF4","colab_type":"text"},"source":["Time Series data must be re-framed as a supervised learning dataset before we can start using machine learning algorithms.  \n","There is no concept of input and output features in time series. Instead, we must choose the variable to be predicted and use feature engineering to construct all of the inputs that will be used to make predictions for future time steps."]},{"cell_type":"markdown","metadata":{"id":"9atRO7l0UOF7","colab_type":"text"},"source":["## Feature Engineering for Times Series"]},{"cell_type":"markdown","metadata":{"id":"ameUwQssUOF8","colab_type":"text"},"source":["[feature_eng_ts.png](attachment:feature_eng_ts.png)"]},{"cell_type":"markdown","metadata":{"id":"Wu-0lspKUOF9","colab_type":"text"},"source":["In this tutorial, we will look at three classes of features that we can create from our time series dataset:\n","\n","    Date Time Features: these are components of the time step itself for each observation.\n","    Lag Features: these are values at prior time steps.\n","    Window Features: these are a summary of values over a fixed window of prior time steps.\n"]},{"cell_type":"markdown","metadata":{"id":"sYbH0vQDUOF_","colab_type":"text"},"source":["Lag features are the classical way that time series forecasting problems are transformed into supervised learning problems."]},{"cell_type":"markdown","metadata":{"id":"kBbg_jTeUOGA","colab_type":"text"},"source":["The goal of feature engineering is to provide strong and ideally simple relationships between new input features and the output feature for the supervised learning algorithm to model."]},{"cell_type":"markdown","metadata":{"id":"eInrwANETsUE","colab_type":"text"},"source":["**Preprocessing for Multioutput Forecasting**"]},{"cell_type":"markdown","metadata":{"id":"PBb7LhJzUcKk","colab_type":"text"},"source":["### Preprocessing (For multi output model)\n","For supervised machine learning methods to work on time series it is necessary to do a certain amount of preprocessing. This includes feature generation such as embedding(lags). "]},{"cell_type":"markdown","metadata":{"id":"ZK9RC9EXNC5A","colab_type":"text"},"source":["Preprocessing for 1 month of training data"]},{"cell_type":"code","metadata":{"id":"NsVVIN2PJMVj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598470466651,"user_tz":-120,"elapsed":618,"user":{"displayName":"Manuel S","photoUrl":"","userId":"12634565971130299273"}},"outputId":"b32d280a-2249-4050-b853-5e627e8d9753"},"source":["##\n","taxi_ts_1month = taxidemand_ts.iloc[7776:-192] \n","taxi_ts_1month.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(792,)"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"code","metadata":{"id":"7R7xnarkTo_Y","colab_type":"code","colab":{}},"source":["## Set paramaters\n","data = taxi_ts_1month.copy()\n","input_lags = 60 ## number of lags to be used for input. should be 2,5 times the seasonal period \n","output_lags = 24 ## number of future oberservations to be forecasted\n","n_test = 24 ## size of test set "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nho3_ThCTpDr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1598470475874,"user_tz":-120,"elapsed":717,"user":{"displayName":"Manuel S","photoUrl":"","userId":"12634565971130299273"}},"outputId":"e4bacd53-7d6b-4ce3-ed46-34695e20681c"},"source":["## Split data in train and test set\n","train = data[0:-n_test]\n","test = data[-n_test:]\n","print(train.shape)\n","print(test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(768,)\n","(24,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Pbs7Q3HkUnfZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598470480374,"user_tz":-120,"elapsed":477,"user":{"displayName":"Manuel S","photoUrl":"","userId":"12634565971130299273"}},"outputId":"e382239e-59d3-476f-c09a-b0c72c3c2159"},"source":["## Create dataframe with the correct dimensions. Each row represents the past 54 observations and the 24 future observations\n","df = pd.DataFrame()\n","n_train = len(train)\n","\n","## create inputs lags\n","for i in range(input_lags,0,-1):\n","    df['t-' + str(i)] = train.shift(i)\n","\n","## create output lags\n","for j in range(0,output_lags,1):\n","    df['t+' + str(j)] = train.shift(-j)\n","\n","## remove the first input_lags rows and last output_lags rows   \n","df = df[input_lags:(n_train-output_lags+1)]  \n","\n","print(df.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(685, 84)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NQqdNzZ1Un3E","colab_type":"code","colab":{}},"source":["## Split train into features X and targets Y\n","X_train = df.iloc[:,:input_lags] \n","y_train = df.iloc[:,input_lags:]\n","\n","## To create the test features X_test we cannot use any of the test set, since that includes the data held out\n","# for testing. We therefor use the last input_lags number of observations in the training set. These are however\n","# split between the targets and features and requires a combination of the two. (output_lags) from the targets \n","# and (input_lags - output_lags) from the features\n","X_test = X_train.iloc[len(X_train) - 1,:][output_lags:] ## First get the last (input_lags - output_lags) from the features \n","X_test = X_test.append(y_train.iloc[len(y_train) - 1,:]) ## Second, add the last (output_lags) from the targets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xhWQdK0yVJq0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1598470486260,"user_tz":-120,"elapsed":955,"user":{"displayName":"Manuel S","photoUrl":"","userId":"12634565971130299273"}},"outputId":"d6143cf3-1927-4a82-aa00-2f3572bc62a1"},"source":["## Remodel as numpy arrays and reshape\n","X_train_multi = X_train.values ## should be (n - input_lags - outputlags - n_test + 1) x (input_lags)\n","y_train_multi = y_train.values ## should be (n - input_lags - outputlags - n_test + 1) x (output_lags)\n","X_test_multi = X_test.values.reshape(1,input_lags) ## should be (1) x (input_lags) \n","y_test_multi = test.values.reshape(1,n_test) ## should be (1) x (n_test)\n","\n","print(\"X_train_multi: \" + \"type: \" + str(type(X_train_multi)) + \"\\tshape: \" + str(X_train_multi.shape))\n","print(\"y_train_multi: \" + \"type: \" + str(type(y_train_multi)) + \"\\tshape: \" + str(y_train_multi.shape))\n","print(\"X_test_multi: \" + \"type: \" + str(type(X_test_multi)) + \"\\tshape: \" + str(X_test_multi.shape))\n","print(\"y_test_multi: \" + \"type: \" + str(type(y_test_multi)) + \"\\tshape: \" + str(y_test_multi.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["X_train_multi: type: <class 'numpy.ndarray'>\tshape: (685, 60)\n","y_train_multi: type: <class 'numpy.ndarray'>\tshape: (685, 24)\n","X_test_multi: type: <class 'numpy.ndarray'>\tshape: (1, 60)\n","y_test_multi: type: <class 'numpy.ndarray'>\tshape: (1, 24)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ugerdqw5Un6N","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pgGZuED8UOJJ","colab_type":"text"},"source":["$\\textbf{General setup of the algorithm}:$ As far as general parameters go, the booster \"gbtree\" has been used here, ie. I have been using tree based models in each iteration instead of a linear model, which is rarely used. I have started out with a number of estimators of 1,000 and then I have fine tuned the following parameters after try 2: max_depth and min_child_weight (gsearch1), reg_alpha (gsearch2), gamma (gsearch3), subsample and colsample_bytree (gsearch4), reg_lambda (gsearch5).  \n","I have used a feature importance plot of the XGBClassfier model to select the most important features to include. I have included the top 10 features out of the 24 features provided from try 3 onwards.\n","The fine tuned parameters have the following influence on the xg boosting algorithm:  \n","  \n","$\\textit{max_depth:}$ specifies the max depth of a tree and can be used to control overfitting as higher depth will allow the model to learn relations very specific to a particular sample   \n","$\\textit{min_child_weight:}$ this sets the minimum sum of weights of all observations required in a child. Higher values prevent the model from learning too specific relations.  \n","$\\textit{reg_alpha:}$ L1 regularization term. Can be used in case of high dimensionality to make the algorithm run faster. Can be a solution to overfitting in case of a relatively small dataset.  \n","$\\textit{gamma:}$ sets the minimum loss function required to make a split.  \n","$\\textit{subsample:}$ sets the fraction of observations to be random samples of each tree. lower values prevent overfitting but small too small values might lead to underfitting.  \n","$\\textit{colsample_bytree:}$ fraction of columns to be random samples of each tree.  \n","$\\textit{reg_lambda:}$ L2 regularization term. can be a solution to overfitting in case of a relatively small dataset. Can be explored to reduce overfitting."]},{"cell_type":"markdown","metadata":{"id":"wkMGlECaoHJx","colab_type":"text"},"source":["#LightGBM (Gradient Boosting)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"eYzGHCojvx2C","colab_type":"text"},"source":["https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html\n","\n","lightgbm parameters:\n","\n","https://lightgbm.readthedocs.io/en/latest/Parameters.html\n"]},{"cell_type":"markdown","metadata":{"id":"2KXZr7NuUOJe","colab_type":"text"},"source":["## Fitting the optimal model"]},{"cell_type":"markdown","metadata":{"id":"hlCnunULw3cd","colab_type":"text"},"source":["#LightGBM optimal model 1 month\n","\n"]},{"cell_type":"code","metadata":{"id":"hm0c9vA6OFYc","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VOzGeyT2OOYw","colab_type":"code","colab":{}},"source":["# best model (1 month)\n","lgbm_reg_opt_new = LGBMRegressor(random_state=1,learning_rate=0.1, objective = \"root_mean_squared_error\", n_estimators =500, colsample_bytree = 0.8, max_depth = 8, subsample= 1.0, num_leaves=30, min_child_samples=20, min_child_weight =3, reg_lambda = 0.7, reg_alpha= 0.0, eval_metric = \"rmse\",boosting_type=\"gbdt\")\n","\n","lgbm_moreg_fit_new = MultiOutputRegressor(lgbm_reg_opt_new).fit(X_train_multi, y_train_multi)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jcNkdfLJUOJ0","colab_type":"text"},"source":["## Obtaining predictions"]},{"cell_type":"markdown","metadata":{"id":"ik5AQRJ9XePJ","colab_type":"text"},"source":["**Multi-output model**"]},{"cell_type":"markdown","metadata":{"id":"jS_xcBTOzKFS","colab_type":"text"},"source":["LIGHTGBM"]},{"cell_type":"code","metadata":{"id":"ZNKPP8JwOZF3","colab_type":"code","colab":{}},"source":["# 1 month\n","lgbm_preds_new = lgbm_moreg_fit_new.predict(X_test_multi)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ySezbidoPHwh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598470582460,"user_tz":-120,"elapsed":602,"user":{"displayName":"Manuel S","photoUrl":"","userId":"12634565971130299273"}},"outputId":"670cfac6-a5f6-47f8-96ba-97d2760f88f5"},"source":["# MSE\n","mse_lgbm_new = mean_squared_error(y_test_multi, lgbm_preds_new)\n","mse_lgbm_new"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["774366.4886961328"]},"metadata":{"tags":[]},"execution_count":63}]},{"cell_type":"code","metadata":{"id":"cUoVb4uUPH0A","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598470584311,"user_tz":-120,"elapsed":520,"user":{"displayName":"Manuel S","photoUrl":"","userId":"12634565971130299273"}},"outputId":"eb65168c-5c79-4505-f49e-e5f6dcd2f5cb"},"source":["# RMSE\n","sqrt(mse_lgbm_new)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["879.9809592804453"]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"code","metadata":{"id":"8HwWNo4aPETm","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mZn0p0E6PPBF","colab_type":"text"},"source":["Preprocessing for 6 months of data"]},{"cell_type":"code","metadata":{"id":"pHLQK0uYPEWs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598524577258,"user_tz":-120,"elapsed":710,"user":{"displayName":"Manuel S","photoUrl":"","userId":"12634565971130299273"}},"outputId":"885ebb9d-7df7-44c5-8e44-9008bbcfec7c"},"source":["## \n","taxi_ts_6months = taxidemand_ts.iloc[4896:-192]\n","taxi_ts_6months.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3672,)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"0AAKGthTPYUq","colab_type":"code","colab":{}},"source":["## Set paramaters\n","input_lags = 60 # 2 and a half times the seasonal period\n","output_lags = 24 # we predict 48 hours ahead\n","n_test = 24 # output_lags  (changed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Axs39YZXPbPu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1598524587084,"user_tz":-120,"elapsed":516,"user":{"displayName":"Manuel S","photoUrl":"","userId":"12634565971130299273"}},"outputId":"ed6690de-48a0-4077-d6e6-eb5f53cd3628"},"source":["## Split data in train and test set\n","train = taxi_ts_6months[0:-n_test]\n","test = taxi_ts_6months[-n_test:]\n","print(train.shape)\n","print(test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(3648,)\n","(24,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CXkScjRpPejO","colab_type":"code","colab":{}},"source":["## Create lagged values for both input and output window (24)\n","data = train.copy()\n","n_train = len(data)\n","\n","##Create lagged values for input\n","df = pd.DataFrame()\n","for i in range(input_lags,0,-1):\n","    df['t-' + str(i)] = data.shift(i)\n","\n","##Create lagged values for output\n","for j in range(0,output_lags,1):\n","    df['t+' + str(j)] = data.shift(-j)\n","    \n","df = df[input_lags:(n_train-output_lags+1)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rMmMHlRYPeqL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1598524592434,"user_tz":-120,"elapsed":650,"user":{"displayName":"Manuel S","photoUrl":"","userId":"12634565971130299273"}},"outputId":"3b16de8c-7dad-43be-ce39-e1db18086645"},"source":["## splitting the training set into labels and features\n","X_train = df.iloc[:,:input_lags] # from the beginning to input_lags\n","Y_train = df.iloc[:,input_lags:] # from input_lags to the end\n","\n","## Use the last window of the training set as the features for the test set. This requires a combination of \n","## X_train and Y_train.\n","X_test = X_train.iloc[len(X_train) - 1,:][output_lags:]\n","X_test = X_test.append(Y_train.iloc[len(Y_train) - 1,:]).values.reshape(1,input_lags)\n","Y_test = test[:output_lags].values.reshape(1,output_lags)\n","\n","X_train = X_train.values # 54 steps back (54 lags)\n","Y_train = Y_train.values # 24 steps ahead\n","\n","print(\"X_train: \" + \"type: \" + str(type(X_train)) + \"\\tshape: \" + str(X_train.shape))\n","print(\"Y_train: \" + \"type: \" + str(type(Y_train)) + \"\\tshape: \" + str(Y_train.shape))\n","print(\"X_test: \" + \"type: \" + str(type(X_test)) + \"\\tshape: \" + str(X_test.shape))\n","print(\"Y_test: \" + \"type: \" + str(type(Y_test)) + \"\\tshape: \" + str(Y_test.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["X_train: type: <class 'numpy.ndarray'>\tshape: (3565, 60)\n","Y_train: type: <class 'numpy.ndarray'>\tshape: (3565, 24)\n","X_test: type: <class 'numpy.ndarray'>\tshape: (1, 60)\n","Y_test: type: <class 'numpy.ndarray'>\tshape: (1, 24)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2AlTYGePPNzW","colab_type":"code","colab":{}},"source":["# best model (6 months)\n","lgbm_reg_opt_new = LGBMRegressor(random_state=1,learning_rate=0.1, objective = \"root_mean_squared_error\", n_estimators =500, colsample_bytree = 0.8, max_depth = 8, subsample= 1.0, num_leaves=30, min_child_samples=20, min_child_weight =3, reg_lambda = 0.7, reg_alpha= 0.0, eval_metric = \"rmse\",boosting_type=\"gbdt\")\n","\n","lgbm_moreg_fit_new = MultiOutputRegressor(lgbm_reg_opt_new).fit(X_train, Y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"He4LJaCdPyeF","colab_type":"text"},"source":["Obtaining predictions"]},{"cell_type":"code","metadata":{"id":"-MhMCy_vPqrV","colab_type":"code","colab":{}},"source":["lgbm_preds_new = lgbm_moreg_fit_new.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ov1uX9yAP6D7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598524688459,"user_tz":-120,"elapsed":510,"user":{"displayName":"Manuel S","photoUrl":"","userId":"12634565971130299273"}},"outputId":"5909a3e6-6f61-4f1f-fbce-d2f45012f1c6"},"source":["# MSE\n","mse_lgbm_new = mean_squared_error(Y_test, lgbm_preds_new)\n","mse_lgbm_new"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["661085.6131704878"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"OmbmiRWHP6BW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598524690833,"user_tz":-120,"elapsed":499,"user":{"displayName":"Manuel S","photoUrl":"","userId":"12634565971130299273"}},"outputId":"a497cc3e-de76-48ee-e0d3-5c6d8c0f518e"},"source":["# RMSE\n","sqrt(mse_lgbm_new)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["813.0717146540567"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"aJrhw2C1Z5IA","colab_type":"code","colab":{}},"source":["# saving the dataframe as a CSV file\n","LGBM_df.to_csv('/gdrive/My Drive/Colab Notebooks/LGBM_preds.csv', index=False)"],"execution_count":null,"outputs":[]}]}