{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"LGBM_emergency_horizon.ipynb","provenance":[{"file_id":"1VWyl_Nokyt_BxSz0hoBw8-VGfPbyE2-v","timestamp":1598519675990},{"file_id":"1uWw2wZyZ8yORKTYJ3fKRfEsKjYvuLcpa","timestamp":1598465352152},{"file_id":"1B3RwPaDkpiRYdoj7BB0XD4fa9lmK3xOk","timestamp":1596382012502}],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"y6YsUR3mUOEy","colab_type":"text"},"source":["# LGBM for the emergency demand predictions"]},{"cell_type":"markdown","metadata":{"id":"3d2qnLn3UOE1","colab_type":"text"},"source":["https://www.kaggle.com/robikscube/tutorial-time-series-forecasting-with-xgboost"]},{"cell_type":"markdown","metadata":{"id":"4jMJD4_iUOE4","colab_type":"text"},"source":["Importing libraries"]},{"cell_type":"code","metadata":{"id":"2KN6E8MKUOE7","colab_type":"code","colab":{}},"source":["# general routine set up:\n","\n","# ignore warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# get wd and change the wd plus import libraries\n","import os\n","os.getcwd()\n","#os.chdir(\"/Users/Manu/Dropbox/CBS MSc Thesis Research Folder/DATA & Code/Model Specific Notebooks\")\n","\n","\n","# standard\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt \n","%matplotlib inline\n","from math import sqrt\n","import pickle\n","\n","# ML\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import TimeSeriesSplit\n","from sklearn.model_selection import train_test_split\n","import xgboost as xgb\n","from xgboost import plot_importance, plot_tree\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","from sklearn.multioutput import MultiOutputRegressor\n","import tensorflow.keras.backend as K\n","from lightgbm import LGBMRegressor"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JDuBnHiZUOFN","colab_type":"text"},"source":["Loading the data"]},{"cell_type":"code","metadata":{"id":"aIizwmU9XRQK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600101713624,"user_tz":-120,"elapsed":26095,"user":{"displayName":"Manuel S","photoUrl":"","userId":"12634565971130299273"}},"outputId":"b44859d3-e991-4f16-f094-7c405ae7e0d7"},"source":["# this allows for accessing files stored in your google drive using the path \"/gdrive/\"\n","# mounting google drive locally:\n","\n","from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9R3DJqolXY_e","colab_type":"code","colab":{}},"source":["#loading the hourly traffic data (1 year of data; June 2018 to June 2019)\n","filename = \"/gdrive/My Drive/Colab Notebooks/emergency_dispatches_bronx_H\"\n","infile = open(filename,'rb')\n","emergency_ts = pickle.load(infile)\n","infile.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VWye-hnAzNq7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598519834306,"user_tz":-120,"elapsed":941,"user":{"displayName":"Manuel S","photoUrl":"","userId":"12634565971130299273"}},"outputId":"346998f8-de1a-437f-c850-1ce6a1776d7f"},"source":["taxidemand_ts.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8760,)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"Z-AofGAaUOFY","colab_type":"text"},"source":["Preprocessing: already done\n"]},{"cell_type":"markdown","metadata":{"id":"exVAFiNZUOF4","colab_type":"text"},"source":["Time Series data must be re-framed as a supervised learning dataset before we can start using machine learning algorithms.  \n","There is no concept of input and output features in time series. Instead, we must choose the variable to be predicted and use feature engineering to construct all of the inputs that will be used to make predictions for future time steps."]},{"cell_type":"markdown","metadata":{"id":"9atRO7l0UOF7","colab_type":"text"},"source":["## Feature Engineering for Times Series"]},{"cell_type":"markdown","metadata":{"id":"ameUwQssUOF8","colab_type":"text"},"source":["[feature_eng_ts.png](attachment:feature_eng_ts.png)"]},{"cell_type":"markdown","metadata":{"id":"Wu-0lspKUOF9","colab_type":"text"},"source":["In this tutorial, we will look at three classes of features that we can create from our time series dataset:\n","\n","    Date Time Features: these are components of the time step itself for each observation.\n","    Lag Features: these are values at prior time steps.\n","    Window Features: these are a summary of values over a fixed window of prior time steps.\n"]},{"cell_type":"markdown","metadata":{"id":"sYbH0vQDUOF_","colab_type":"text"},"source":["Lag features are the classical way that time series forecasting problems are transformed into supervised learning problems."]},{"cell_type":"markdown","metadata":{"id":"kBbg_jTeUOGA","colab_type":"text"},"source":["The goal of feature engineering is to provide strong and ideally simple relationships between new input features and the output feature for the supervised learning algorithm to model."]},{"cell_type":"markdown","metadata":{"id":"eInrwANETsUE","colab_type":"text"},"source":["**Preprocessing for Multioutput Forecasting**"]},{"cell_type":"markdown","metadata":{"id":"PBb7LhJzUcKk","colab_type":"text"},"source":["### Preprocessing (For multi output model)\n","For supervised machine learning methods to work on time series it is necessary to do a certain amount of preprocessing. This includes feature generation such as embedding(lags). "]},{"cell_type":"markdown","metadata":{"id":"ZK9RC9EXNC5A","colab_type":"text"},"source":["Preprocessing for 48 steps ahead"]},{"cell_type":"code","metadata":{"id":"NsVVIN2PJMVj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600101717738,"user_tz":-120,"elapsed":984,"user":{"displayName":"Manuel S","photoUrl":"","userId":"12634565971130299273"}},"outputId":"c3d266e4-0edf-4b9f-909a-f8a4053efdd2"},"source":["## \n","emergency_ts_48 = emergency_ts.iloc[0:-360 + 24]\n","emergency_ts_48.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8424,)"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"7R7xnarkTo_Y","colab_type":"code","colab":{}},"source":["## Set paramaters\n","data = emergency_ts_48.copy()\n","input_lags = 60 ## number of lags to be used for input. should be 2,5 times the seasonal period \n","output_lags = 48 ## number of future oberservations to be forecasted\n","n_test = 48 ## size of test set \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nho3_ThCTpDr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1600101723787,"user_tz":-120,"elapsed":1108,"user":{"displayName":"Manuel S","photoUrl":"","userId":"12634565971130299273"}},"outputId":"1e3fb495-0847-4237-fcd7-b7a7edd2d13f"},"source":["## Split data in train and test set\n","train = data[0:-n_test]\n","test = data[-n_test:]\n","print(train.shape)\n","print(test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(8376,)\n","(48,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Pbs7Q3HkUnfZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600101726529,"user_tz":-120,"elapsed":1023,"user":{"displayName":"Manuel S","photoUrl":"","userId":"12634565971130299273"}},"outputId":"c6aa90ab-a57b-4361-9aeb-b684dddab221"},"source":["## Create dataframe with the correct dimensions. Each row represents the past 54 observations and the 24 future observations\n","df = pd.DataFrame()\n","n_train = len(train)\n","\n","## create inputs lags\n","for i in range(input_lags,0,-1):\n","    df['t-' + str(i)] = train.shift(i)\n","\n","## create output lags\n","for j in range(0,output_lags,1):\n","    df['t+' + str(j)] = train.shift(-j)\n","\n","## remove the first input_lags rows and last output_lags rows   \n","df = df[input_lags:(n_train-output_lags+1)]  \n","\n","print(df.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(8269, 108)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NQqdNzZ1Un3E","colab_type":"code","colab":{}},"source":["## Split train into features X and targets Y\n","X_train = df.iloc[:,:input_lags] \n","y_train = df.iloc[:,input_lags:]\n","\n","## To create the test features X_test we cannot use any of the test set, since that includes the data held out\n","# for testing. We therefor use the last input_lags number of observations in the training set. These are however\n","# split between the targets and features and requires a combination of the two. (output_lags) from the targets \n","# and (input_lags - output_lags) from the features\n","X_test = X_train.iloc[len(X_train) - 1,:][output_lags:] ## First get the last (input_lags - output_lags) from the features \n","X_test = X_test.append(y_train.iloc[len(y_train) - 1,:]) ## Second, add the last (output_lags) from the targets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xhWQdK0yVJq0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1600101729200,"user_tz":-120,"elapsed":461,"user":{"displayName":"Manuel S","photoUrl":"","userId":"12634565971130299273"}},"outputId":"7ecb62e2-a13d-4400-8ec7-7b300218f51b"},"source":["## Remodel as numpy arrays and reshape\n","X_train_multi = X_train.values ## should be (n - input_lags - outputlags - n_test + 1) x (input_lags)\n","y_train_multi = y_train.values ## should be (n - input_lags - outputlags - n_test + 1) x (output_lags)\n","X_test_multi = X_test.values.reshape(1,input_lags) ## should be (1) x (input_lags) \n","y_test_multi = test.values.reshape(1,n_test) ## should be (1) x (n_test)\n","\n","print(\"X_train_multi: \" + \"type: \" + str(type(X_train_multi)) + \"\\tshape: \" + str(X_train_multi.shape))\n","print(\"y_train_multi: \" + \"type: \" + str(type(y_train_multi)) + \"\\tshape: \" + str(y_train_multi.shape))\n","print(\"X_test_multi: \" + \"type: \" + str(type(X_test_multi)) + \"\\tshape: \" + str(X_test_multi.shape))\n","print(\"y_test_multi: \" + \"type: \" + str(type(y_test_multi)) + \"\\tshape: \" + str(y_test_multi.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["X_train_multi: type: <class 'numpy.ndarray'>\tshape: (8269, 60)\n","y_train_multi: type: <class 'numpy.ndarray'>\tshape: (8269, 48)\n","X_test_multi: type: <class 'numpy.ndarray'>\tshape: (1, 60)\n","y_test_multi: type: <class 'numpy.ndarray'>\tshape: (1, 48)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ugerdqw5Un6N","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pgGZuED8UOJJ","colab_type":"text"},"source":["$\\textbf{General setup of the algorithm}:$ As far as general parameters go, the booster \"gbtree\" has been used here, ie. I have been using tree based models in each iteration instead of a linear model, which is rarely used. I have started out with a number of estimators of 1,000 and then I have fine tuned the following parameters after try 2: max_depth and min_child_weight (gsearch1), reg_alpha (gsearch2), gamma (gsearch3), subsample and colsample_bytree (gsearch4), reg_lambda (gsearch5).  \n","I have used a feature importance plot of the XGBClassfier model to select the most important features to include. I have included the top 10 features out of the 24 features provided from try 3 onwards.\n","The fine tuned parameters have the following influence on the xg boosting algorithm:  \n","  \n","$\\textit{max_depth:}$ specifies the max depth of a tree and can be used to control overfitting as higher depth will allow the model to learn relations very specific to a particular sample   \n","$\\textit{min_child_weight:}$ this sets the minimum sum of weights of all observations required in a child. Higher values prevent the model from learning too specific relations.  \n","$\\textit{reg_alpha:}$ L1 regularization term. Can be used in case of high dimensionality to make the algorithm run faster. Can be a solution to overfitting in case of a relatively small dataset.  \n","$\\textit{gamma:}$ sets the minimum loss function required to make a split.  \n","$\\textit{subsample:}$ sets the fraction of observations to be random samples of each tree. lower values prevent overfitting but small too small values might lead to underfitting.  \n","$\\textit{colsample_bytree:}$ fraction of columns to be random samples of each tree.  \n","$\\textit{reg_lambda:}$ L2 regularization term. can be a solution to overfitting in case of a relatively small dataset. Can be explored to reduce overfitting."]},{"cell_type":"markdown","metadata":{"id":"wkMGlECaoHJx","colab_type":"text"},"source":["#LightGBM (Gradient Boosting)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"eYzGHCojvx2C","colab_type":"text"},"source":["https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html\n","\n","lightgbm parameters:\n","\n","https://lightgbm.readthedocs.io/en/latest/Parameters.html\n"]},{"cell_type":"markdown","metadata":{"id":"EQL8P44CBMGh","colab_type":"text"},"source":["optimal grid (August 25) :\n","{'estimator__colsample_bytree': 0.8,\n"," 'estimator__learning_rate': 0.1,\n"," 'estimator__max_depth': 8,\n"," 'estimator__min_child_samples': 30,\n"," 'estimator__min_child_weight': 0.01,\n"," 'estimator__n_estimators': 250,\n"," 'estimator__num_leaves': 30,\n"," 'estimator__reg_alpha': 0.0,\n"," 'estimator__reg_lambda': 0.7,\n"," 'estimator__subsample': 1.0}"]},{"cell_type":"markdown","metadata":{"id":"2KXZr7NuUOJe","colab_type":"text"},"source":["## Fitting the optimal model"]},{"cell_type":"markdown","metadata":{"id":"hlCnunULw3cd","colab_type":"text"},"source":["#LightGBM optimal model 48 step ahead predictions\n","\n"]},{"cell_type":"code","metadata":{"id":"hm0c9vA6OFYc","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VOzGeyT2OOYw","colab_type":"code","colab":{}},"source":["# training the best model (48 steps ahead)\n","lgbm_reg_opt_new = LGBMRegressor(random_state=1,learning_rate=0.01, objective = \"root_mean_squared_error\", n_estimators =250, colsample_bytree = 0.8, max_depth = 15, subsample= 1.0, num_leaves=150, min_child_samples=20, reg_lambda = 0.0, reg_alpha= 0.0, eval_metric = \"rmse\",boosting_type=\"gbdt\")\n","lgbm_moreg_fit_new = MultiOutputRegressor(lgbm_reg_opt_new).fit(X_train_multi, y_train_multi)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jcNkdfLJUOJ0","colab_type":"text"},"source":["## Obtaining predictions"]},{"cell_type":"markdown","metadata":{"id":"ik5AQRJ9XePJ","colab_type":"text"},"source":["**Multi-output model**"]},{"cell_type":"markdown","metadata":{"id":"jS_xcBTOzKFS","colab_type":"text"},"source":["LIGHTGBM"]},{"cell_type":"code","metadata":{"id":"ZNKPP8JwOZF3","colab_type":"code","colab":{}},"source":["# 48 steps ahead predictions\n","lgbm_preds_new = lgbm_moreg_fit_new.predict(X_test_multi)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ySezbidoPHwh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600101854618,"user_tz":-120,"elapsed":1026,"user":{"displayName":"Manuel S","photoUrl":"","userId":"12634565971130299273"}},"outputId":"166fcf8a-47cc-4b7a-e276-232ba5edcc89"},"source":["# MSE\n","mse_lgbm_new = mean_squared_error(y_test_multi, lgbm_preds_new)\n","mse_lgbm_new"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["38.12179973450926"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"cUoVb4uUPH0A","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600101855033,"user_tz":-120,"elapsed":1426,"user":{"displayName":"Manuel S","photoUrl":"","userId":"12634565971130299273"}},"outputId":"7a6e1fde-8613-4082-c165-03b1e75040ff"},"source":["# RMSE\n","sqrt(mse_lgbm_new)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6.174285362251186"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"mZn0p0E6PPBF","colab_type":"text"},"source":["LGBM predictions 72 steps ahead"]},{"cell_type":"code","metadata":{"id":"pHLQK0uYPEWs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600101865602,"user_tz":-120,"elapsed":982,"user":{"displayName":"Manuel S","photoUrl":"","userId":"12634565971130299273"}},"outputId":"44fd7416-32e0-464c-abf9-a69982b31b80"},"source":["## Remove last 24 hours of data since they are erroneous\n","emergency_ts_72 = emergency_ts.iloc[0:-360 + 24*2]\n","emergency_ts_72.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8448,)"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"0AAKGthTPYUq","colab_type":"code","colab":{}},"source":["## Set paramaters\n","data = emergency_ts_72.copy()\n","input_lags = 60 # 2 and a half times the seasonal period\n","output_lags = 72 # we predict 72 hours ahead\n","n_test = 72 # output_lags  (changed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Axs39YZXPbPu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1600101868846,"user_tz":-120,"elapsed":1254,"user":{"displayName":"Manuel S","photoUrl":"","userId":"12634565971130299273"}},"outputId":"38954700-823a-4590-b15c-4b263bcfab01"},"source":["## Split data in train and test set\n","train = data[0:-n_test]\n","test = data[-n_test:]\n","print(train.shape)\n","print(test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(8376,)\n","(72,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CXkScjRpPejO","colab_type":"code","colab":{}},"source":["## Create lagged values for both input and output window (24)\n","data = train.copy()\n","n_train = len(data)\n","\n","##Create lagged values for input\n","df = pd.DataFrame()\n","for i in range(input_lags,0,-1):\n","    df['t-' + str(i)] = data.shift(i)\n","\n","##Create lagged values for output\n","for j in range(0,output_lags,1):\n","    df['t+' + str(j)] = data.shift(-j)\n","    \n","df = df[input_lags:(n_train-output_lags+1)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rMmMHlRYPeqL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1600101874944,"user_tz":-120,"elapsed":1047,"user":{"displayName":"Manuel S","photoUrl":"","userId":"12634565971130299273"}},"outputId":"5cf356a4-f80f-441b-bded-6941d28ef540"},"source":["## splitting the training set into labels and features\n","X_train = df.iloc[:,:input_lags] # from the beginning to input_lags\n","Y_train = df.iloc[:,input_lags:] # from input_lags to the end\n","\n","## Use the last window of the training set as the features for the test set. This requires a combination of \n","## X_train and Y_train.\n","#X_test = X_train.iloc[len(X_train) - 1,:][output_lags:]\n","#X_test = X_test.append(Y_train.iloc[len(Y_train) - 1,:]).values.reshape(1,input_lags)\n","X_test = Y_train.iloc[len(Y_train) - 1,:][-input_lags:].values.reshape(1,input_lags)\n","Y_test = test[:output_lags].values.reshape(1,output_lags)\n","\n","X_train = X_train.values # 54 steps back (54 lags)\n","Y_train = Y_train.values # 24 steps ahead\n","\n","print(\"X_train: \" + \"type: \" + str(type(X_train)) + \"\\tshape: \" + str(X_train.shape))\n","print(\"Y_train: \" + \"type: \" + str(type(Y_train)) + \"\\tshape: \" + str(Y_train.shape))\n","print(\"X_test: \" + \"type: \" + str(type(X_test)) + \"\\tshape: \" + str(X_test.shape))\n","print(\"Y_test: \" + \"type: \" + str(type(Y_test)) + \"\\tshape: \" + str(Y_test.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["X_train: type: <class 'numpy.ndarray'>\tshape: (8245, 60)\n","Y_train: type: <class 'numpy.ndarray'>\tshape: (8245, 72)\n","X_test: type: <class 'numpy.ndarray'>\tshape: (1, 60)\n","Y_test: type: <class 'numpy.ndarray'>\tshape: (1, 72)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2AlTYGePPNzW","colab_type":"code","colab":{}},"source":["# best model (72 steps ahead)\n","lgbm_reg_opt_new = LGBMRegressor(random_state=1,learning_rate=0.01, objective = \"root_mean_squared_error\", n_estimators =250, colsample_bytree = 0.8, max_depth = 15, subsample= 1.0, num_leaves=150, min_child_samples=20, reg_lambda = 0.0, reg_alpha= 0.0, eval_metric = \"rmse\",boosting_type=\"gbdt\")\n","lgbm_moreg_fit_new = MultiOutputRegressor(lgbm_reg_opt_new).fit(X_train, Y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"He4LJaCdPyeF","colab_type":"text"},"source":["\n","\n","\n","\n","Obtaining predictions 72 hours ahead\n"]},{"cell_type":"code","metadata":{"id":"-MhMCy_vPqrV","colab_type":"code","colab":{}},"source":["lgbm_preds_new = lgbm_moreg_fit_new.predict(X_test_multi)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ov1uX9yAP6D7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600101992170,"user_tz":-120,"elapsed":104100,"user":{"displayName":"Manuel S","photoUrl":"","userId":"12634565971130299273"}},"outputId":"54f50908-9a85-4b30-eb3d-ab57ff8ffae3"},"source":["# MSE\n","mse_lgbm_new = mean_squared_error(Y_test, lgbm_preds_new)\n","mse_lgbm_new"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["36.9177299557323"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"OmbmiRWHP6BW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600101992171,"user_tz":-120,"elapsed":102962,"user":{"displayName":"Manuel S","photoUrl":"","userId":"12634565971130299273"}},"outputId":"434b82d5-85a7-40ef-e045-e044151e710f"},"source":["# RMSE\n","sqrt(mse_lgbm_new)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6.075996210970864"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"XOB_qHP2V2og","colab_type":"text"},"source":["96 hours ahead predictions"]},{"cell_type":"code","metadata":{"id":"mzEpx0GYV5ok","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600102044068,"user_tz":-120,"elapsed":964,"user":{"displayName":"Manuel S","photoUrl":"","userId":"12634565971130299273"}},"outputId":"31e60c0a-970b-4b7f-d53f-235de92afd18"},"source":["## Remove last 24 hours of data since they are erroneous\n","emergency_ts_96 = emergency_ts.iloc[0:-360 + 24*3]\n","emergency_ts_96.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8472,)"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"EBxDWQcQV5mU","colab_type":"code","colab":{}},"source":["## Set paramaters\n","data = emergency_ts_96.copy()\n","input_lags = 60 # 2 and a half times the seasonal period\n","output_lags = 96 # we predict 96 hours ahead\n","n_test = 96 # output_lags  (changed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rgZinLvrWN5O","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1600102046412,"user_tz":-120,"elapsed":995,"user":{"displayName":"Manuel S","photoUrl":"","userId":"12634565971130299273"}},"outputId":"e10696a0-01ab-4ded-8f59-8c79d6de1043"},"source":["## Split data in train and test set\n","train = emergency_ts_96[0:-n_test]\n","test = emergency_ts_96[-n_test:]\n","print(train.shape)\n","print(test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(8376,)\n","(96,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YmUdRQiHWppp","colab_type":"code","colab":{}},"source":["## Create lagged values for both input and output window (24)\n","data = train.copy()\n","n_train = len(data)\n","\n","##Create lagged values for input\n","df = pd.DataFrame()\n","for i in range(input_lags,0,-1):\n","    df['t-' + str(i)] = data.shift(i)\n","\n","##Create lagged values for output\n","for j in range(0,output_lags,1):\n","    df['t+' + str(j)] = data.shift(-j)\n","    \n","df = df[input_lags:(n_train-output_lags+1)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YGfA7KeXWti0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1600102060269,"user_tz":-120,"elapsed":1087,"user":{"displayName":"Manuel S","photoUrl":"","userId":"12634565971130299273"}},"outputId":"d2d4ec3b-a6f3-4c34-a2b6-52a3e09a40f8"},"source":["## splitting the training set into labels and features\n","X_train = df.iloc[:,:input_lags] # from the beginning to input_lags\n","Y_train = df.iloc[:,input_lags:] # from input_lags to the end\n","\n","## Use the last window of the training set as the features for the test set. This requires a combination of \n","## X_train and Y_train.\n","#X_test = X_train.iloc[len(X_train) - 1,:][output_lags:]\n","#X_test = X_test.append(Y_train.iloc[len(Y_train) - 1,:]).values.reshape(1,input_lags)\n","X_test = Y_train.iloc[len(Y_train) - 1,:][-input_lags:].values.reshape(1,input_lags)\n","Y_test = test[:output_lags].values.reshape(1,output_lags)\n","\n","X_train = X_train.values # 54 steps back (54 lags)\n","Y_train = Y_train.values # 24 steps ahead\n","\n","print(\"X_train: \" + \"type: \" + str(type(X_train)) + \"\\tshape: \" + str(X_train.shape))\n","print(\"Y_train: \" + \"type: \" + str(type(Y_train)) + \"\\tshape: \" + str(Y_train.shape))\n","print(\"X_test: \" + \"type: \" + str(type(X_test)) + \"\\tshape: \" + str(X_test.shape))\n","print(\"Y_test: \" + \"type: \" + str(type(Y_test)) + \"\\tshape: \" + str(Y_test.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["X_train: type: <class 'numpy.ndarray'>\tshape: (8221, 60)\n","Y_train: type: <class 'numpy.ndarray'>\tshape: (8221, 96)\n","X_test: type: <class 'numpy.ndarray'>\tshape: (1, 60)\n","Y_test: type: <class 'numpy.ndarray'>\tshape: (1, 96)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GNq6w2iKWtgz","colab_type":"code","colab":{}},"source":["# best model (96 steps ahead)\n","lgbm_reg_opt_new = LGBMRegressor(random_state=1,learning_rate=0.01, objective = \"root_mean_squared_error\", n_estimators =250, colsample_bytree = 0.8, max_depth = 15, subsample= 1.0, num_leaves=150, min_child_samples=20, reg_lambda = 0.0, reg_alpha= 0.0, eval_metric = \"rmse\",boosting_type=\"gbdt\")\n","\n","lgbm_moreg_fit_new = MultiOutputRegressor(lgbm_reg_opt_new).fit(X_train, Y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GYRnhKGZW6wy","colab_type":"text"},"source":["Obtaining predictions 96 hours ahead"]},{"cell_type":"code","metadata":{"id":"7AzJZKC_XAZt","colab_type":"code","colab":{}},"source":["lgbm_preds_new = lgbm_moreg_fit_new.predict(X_test_multi)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"89pkeZXmXIB2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600102209968,"user_tz":-120,"elapsed":135109,"user":{"displayName":"Manuel S","photoUrl":"","userId":"12634565971130299273"}},"outputId":"57530d8b-b737-4f37-86f4-a01472bcf981"},"source":["# MSE\n","mse_lgbm_new = mean_squared_error(Y_test, lgbm_preds_new)\n","mse_lgbm_new"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["35.833427611431006"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"oDHhrkeiXK1R","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600102209969,"user_tz":-120,"elapsed":134144,"user":{"displayName":"Manuel S","photoUrl":"","userId":"12634565971130299273"}},"outputId":"0b2e4316-815b-4601-c08c-04aaf4ffdc4f"},"source":["# RMSE\n","sqrt(mse_lgbm_new)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5.986102873442037"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"SiueiZQqZvF0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":802},"executionInfo":{"status":"ok","timestamp":1597228113470,"user_tz":-120,"elapsed":1181,"user":{"displayName":"Manuel S","photoUrl":"","userId":"12634565971130299273"}},"outputId":"7a75a9a1-a6e6-4787-9b33-d8ac59b7c996"},"source":["LGBM_df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>LGBM predictions</th>\n","      <th>PI upper bound</th>\n","      <th>PI lower bound</th>\n","      <th>Test data</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>10775.684187</td>\n","      <td>11989.102104</td>\n","      <td>10052.530400</td>\n","      <td>11284</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>8785.240102</td>\n","      <td>10255.177836</td>\n","      <td>6991.671354</td>\n","      <td>9431</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>7417.886066</td>\n","      <td>8736.246831</td>\n","      <td>4155.632162</td>\n","      <td>7524</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4760.588988</td>\n","      <td>6079.624763</td>\n","      <td>2193.597025</td>\n","      <td>5155</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3079.153578</td>\n","      <td>4226.169554</td>\n","      <td>1730.670195</td>\n","      <td>3075</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>2349.560298</td>\n","      <td>4917.127980</td>\n","      <td>2034.047477</td>\n","      <td>1790</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>4080.604715</td>\n","      <td>8291.035464</td>\n","      <td>3362.204861</td>\n","      <td>2571</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>6213.181292</td>\n","      <td>11857.649223</td>\n","      <td>4217.161542</td>\n","      <td>3975</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>8325.297125</td>\n","      <td>14743.700593</td>\n","      <td>6441.890914</td>\n","      <td>5945</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9135.712462</td>\n","      <td>13749.349079</td>\n","      <td>8374.541383</td>\n","      <td>8115</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>10329.995873</td>\n","      <td>13421.766107</td>\n","      <td>9696.186442</td>\n","      <td>9430</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>10776.887206</td>\n","      <td>13719.276303</td>\n","      <td>9837.185809</td>\n","      <td>10589</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>12007.038879</td>\n","      <td>13818.234464</td>\n","      <td>10508.890650</td>\n","      <td>11434</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>12077.096580</td>\n","      <td>14321.780740</td>\n","      <td>11205.071642</td>\n","      <td>12003</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>12197.663182</td>\n","      <td>14547.323417</td>\n","      <td>11357.651073</td>\n","      <td>11607</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>12189.726669</td>\n","      <td>14763.177256</td>\n","      <td>11376.633704</td>\n","      <td>11612</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>12081.140169</td>\n","      <td>14350.543139</td>\n","      <td>11045.824071</td>\n","      <td>11254</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>12827.527050</td>\n","      <td>15154.851897</td>\n","      <td>11436.748550</td>\n","      <td>12367</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>13106.044611</td>\n","      <td>16531.576070</td>\n","      <td>12577.660930</td>\n","      <td>13028</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>12697.500898</td>\n","      <td>16684.157226</td>\n","      <td>11129.450100</td>\n","      <td>12649</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>11287.574838</td>\n","      <td>15685.105702</td>\n","      <td>10312.168082</td>\n","      <td>10736</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>10809.567242</td>\n","      <td>15023.252373</td>\n","      <td>9720.171288</td>\n","      <td>10975</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>11735.657511</td>\n","      <td>15182.775099</td>\n","      <td>9045.518445</td>\n","      <td>12467</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>11973.188690</td>\n","      <td>14334.097295</td>\n","      <td>7374.200768</td>\n","      <td>12054</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    LGBM predictions  PI upper bound  PI lower bound  Test data\n","0       10775.684187    11989.102104    10052.530400      11284\n","1        8785.240102    10255.177836     6991.671354       9431\n","2        7417.886066     8736.246831     4155.632162       7524\n","3        4760.588988     6079.624763     2193.597025       5155\n","4        3079.153578     4226.169554     1730.670195       3075\n","5        2349.560298     4917.127980     2034.047477       1790\n","6        4080.604715     8291.035464     3362.204861       2571\n","7        6213.181292    11857.649223     4217.161542       3975\n","8        8325.297125    14743.700593     6441.890914       5945\n","9        9135.712462    13749.349079     8374.541383       8115\n","10      10329.995873    13421.766107     9696.186442       9430\n","11      10776.887206    13719.276303     9837.185809      10589\n","12      12007.038879    13818.234464    10508.890650      11434\n","13      12077.096580    14321.780740    11205.071642      12003\n","14      12197.663182    14547.323417    11357.651073      11607\n","15      12189.726669    14763.177256    11376.633704      11612\n","16      12081.140169    14350.543139    11045.824071      11254\n","17      12827.527050    15154.851897    11436.748550      12367\n","18      13106.044611    16531.576070    12577.660930      13028\n","19      12697.500898    16684.157226    11129.450100      12649\n","20      11287.574838    15685.105702    10312.168082      10736\n","21      10809.567242    15023.252373     9720.171288      10975\n","22      11735.657511    15182.775099     9045.518445      12467\n","23      11973.188690    14334.097295     7374.200768      12054"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"aJrhw2C1Z5IA","colab_type":"code","colab":{}},"source":["# saving the dataframe as a CSV file\n","LGBM_df.to_csv('/gdrive/My Drive/Colab Notebooks/LGBM_preds.csv', index=False)"],"execution_count":null,"outputs":[]}]}